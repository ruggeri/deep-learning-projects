{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "NUM_EXAMPLES = 1000\n",
    "NUM_FEATURES = 20\n",
    "\n",
    "CLASS0_PROBS = np.random.uniform(size = NUM_FEATURES)\n",
    "CLASS0_ODDS = CLASS0_PROBS / (1 - CLASS0_PROBS)\n",
    "LOG_ODDS_RATIOS = 2 * np.random.uniform(low = -1, high = +1, size = NUM_FEATURES)\n",
    "CLASS1_ODDS = CLASS0_ODDS * np.exp(LOG_ODDS_RATIOS)\n",
    "CLASS1_PROBS = CLASS1_ODDS / (1 + CLASS1_ODDS)\n",
    "\n",
    "training_x = []\n",
    "training_y = []\n",
    "for _ in range(NUM_EXAMPLES):\n",
    "    y = np.random.uniform() > 0.50\n",
    "    \n",
    "    if y == 1:\n",
    "        x = np.random.binomial(1, CLASS1_PROBS, NUM_FEATURES)\n",
    "    else:\n",
    "        x = np.random.binomial(1, CLASS0_PROBS, NUM_FEATURES)\n",
    "    training_x.append(x)\n",
    "    training_y.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1]),\n",
       " array([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1]),\n",
       " array([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1]),\n",
       " array([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1]),\n",
       " array([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1]),\n",
       " array([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1]),\n",
       " array([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1]),\n",
       " array([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1]),\n",
       " array([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0]),\n",
       " array([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "20\n",
      "11.893\n",
      "486\n",
      "514\n"
     ]
    }
   ],
   "source": [
    "# How many examples are there?\n",
    "# How many features are there per example?\n",
    "# How many present features does the average example have?\n",
    "# * Loop through, summing for each example, and then average.\n",
    "# How many positive and negative examples are there?\n",
    "\n",
    "print(len(training_x))\n",
    "print(len(training_x[0]))\n",
    "print(\n",
    "    sum([sum(x) for x in training_x]) / len(training_x)\n",
    ")\n",
    "print(sum(training_y))\n",
    "print(len(training_y) - sum(training_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.178747493114189, -2.3679080715482272, -0.7738539530021048, 1.5037886406277452, -3.1659122084456701, -1.3317132325566838, 0.74767881577146877, 2.0075221809826713, -0.15400633320381951, 3.5089838178742587]\n"
     ]
    }
   ],
   "source": [
    "IDEAL_BIAS = 0 + np.sum(np.log((1 - CLASS1_PROBS) / (1 - CLASS0_PROBS)))\n",
    "IDEAL_THETAS = np.log(\n",
    "    (CLASS1_PROBS / CLASS0_PROBS)\n",
    "    /\n",
    "    ((1 - CLASS1_PROBS) / (1 - CLASS0_PROBS))\n",
    ")\n",
    "\n",
    "# Use the ideal theta to sum up a score for every example. Give me this as an array called \"z_scores\"\n",
    "# Define a function called z_score(example, theta)\n",
    "\n",
    "def z_score(example, thetas, bias):\n",
    "    z = bias\n",
    "    for i in range(NUM_FEATURES):\n",
    "        z += example[i] * thetas[i]\n",
    "    return z\n",
    "\n",
    "z_scores = [\n",
    "    z_score(example, IDEAL_THETAS, IDEAL_BIAS) for example in training_x\n",
    "]\n",
    "\n",
    "# Print out the first ten z scores.\n",
    "print(z_scores[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.819"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a classify function that takes in the set of x values and outputs the predictions based on z>0 or not.\n",
    "# Write a function that takes in a list of predictions and true ys and calculates the error.\n",
    "\n",
    "def classify(examples, thetas, bias):\n",
    "    predictions = []\n",
    "    for idx, example in enumerate(examples):\n",
    "        z = z_score(example, thetas, bias)\n",
    "        if z > 0.0:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions\n",
    "\n",
    "def accuracy(predictions, targets):\n",
    "    num_correct = sum([\n",
    "        1 if predictions[idx] == targets[idx]\n",
    "        else 0\n",
    "        for idx in range(len(predictions))\n",
    "    ])\n",
    "    \n",
    "    return num_correct / len(predictions)\n",
    "\n",
    "accuracy(\n",
    "    classify(training_x, IDEAL_THETAS, IDEAL_BIAS),\n",
    "    training_y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.819"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see a way to turn the z values into probabilities.\n",
    "# We'll use (e^z / (1 + e^z)). This is called the logistic function.\n",
    "\n",
    "def logistic(z):\n",
    "    return (np.exp(z) / (1 + np.exp(z)))\n",
    "\n",
    "# Calculate the probabilities for every example.\n",
    "\n",
    "example_probs = list(map(logistic, z_scores))\n",
    "\n",
    "# Write a function classify2 that classifies as positive if prob > 0.50.\n",
    "# Calculate the accuracy error.\n",
    "# Compare to the result of the original classify. Why are they the same?\n",
    "\n",
    "def classify2(probabilities):\n",
    "    return [1 if prob > 0.50 else 0 for prob in probabilities]\n",
    "\n",
    "accuracy(\n",
    "    classify2(example_probs),\n",
    "    training_y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25854523218263092"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function called mean_absolute_error. This takes in\n",
    "# a list of probabilities and of target values.\n",
    "# Sum the absolute difference and average.\n",
    "\n",
    "def mean_absolute_error(probabilities, targets):\n",
    "    return sum([\n",
    "        np.abs(targets[idx] - probabilities[idx])\n",
    "        for idx in range(len(probabilities))\n",
    "    ]) / len(probabilities)\n",
    "\n",
    "mean_absolute_error(example_probs, training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
      "0.514\n",
      "0.486\n"
     ]
    }
   ],
   "source": [
    "# Calculate the z_scores for if you had only zero theta values.\n",
    "# What values do you get? Why?\n",
    "# Calculate the probabilities. What are they? Why.\n",
    "# Calculate the predictions. Use either classify or classify2. What do you get? Why?\n",
    "# Calculate the accuracy. What do you get. Why?\n",
    "# Calculate the mean_absolute_error. What is this? Why?\n",
    "\n",
    "def zero_thetas():\n",
    "    return [0 for _ in range(NUM_FEATURES)]\n",
    "ZERO_BIAS = 0.0\n",
    "\n",
    "z_scores = [\n",
    "    z_score(example, zero_thetas(), ZERO_BIAS)\n",
    "    for\n",
    "    example in training_x\n",
    "]\n",
    "print(z_scores[:10])\n",
    "probs = list(map(logistic, z_scores))\n",
    "print(probs[:10])\n",
    "predictions = classify2(probs)\n",
    "print(accuracy(predictions, training_y))\n",
    "print(mean_absolute_error(predictions, training_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24145476781736908"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a function called compare_thetas. It should take in the examples,\n",
    "# plus an old set of thetas, and a new one. Compare the errors on one versus the other.\n",
    "# Return the difference in errors.\n",
    "\n",
    "def compare_thetas(examples, training_y, old_thetas, old_bias, new_thetas, new_bias):\n",
    "    old_probs = [\n",
    "        logistic(z_score(example, old_thetas, old_bias))\n",
    "        for\n",
    "        example in examples\n",
    "    ]\n",
    "    new_probs = [\n",
    "        logistic(z_score(example, new_thetas, new_bias))\n",
    "        for\n",
    "        example in examples\n",
    "    ]\n",
    "    \n",
    "    old_error = mean_absolute_error(old_probs, training_y)\n",
    "    new_error = mean_absolute_error(new_probs, training_y)\n",
    "    return new_error - old_error\n",
    "\n",
    "# Compare the ZERO_THETAS and the IDEAL_THETAS.\n",
    "# What sign represents improvement?\n",
    "compare_thetas(training_x, training_y, IDEAL_THETAS, IDEAL_BIAS, zero_thetas(), ZERO_BIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function called `tweak_theta_at_idx`.\n",
    "# It tries to tweak a parameter up or down, to see which one yields improvement.\n",
    "# If neither does, it leaves it.\n",
    "# Do not modify the original thetas array! Return a new tweaked thetas array.\n",
    "\n",
    "def tweak_theta_at_idx(examples, training_y, old_thetas, old_bias, idx, step_size):\n",
    "    new_thetas = list(old_thetas)\n",
    "    new_thetas[idx] += step_size\n",
    "    if compare_thetas(examples, training_y, old_thetas, old_bias, new_thetas, old_bias) < 0.0:\n",
    "        return new_thetas\n",
    "\n",
    "    new_thetas = list(old_thetas)\n",
    "    new_thetas[idx] -= step_size\n",
    "    if compare_thetas(examples, training_y, old_thetas, old_bias, new_thetas, old_bias) < 0.0:\n",
    "        return new_thetas\n",
    "\n",
    "    return old_thetas\n",
    "\n",
    "def tweak_bias(examples, training_y, old_thetas, old_bias, step_size):\n",
    "    new_bias = old_bias + step_size\n",
    "    if compare_thetas(examples, training_y, old_thetas, old_bias, old_thetas, new_bias) < 0.0:\n",
    "        return new_bias\n",
    "\n",
    "    new_bias = old_bias - step_size\n",
    "    if compare_thetas(examples, training_y, old_thetas, old_bias, old_thetas, new_bias) < 0.0:\n",
    "        return new_bias\n",
    "\n",
    "    return new_bias\n",
    "\n",
    "# Write a second function called tweak_all_thetas.\n",
    "def tweak_all_thetas(examples, training_y, thetas, bias, step_size):\n",
    "    for idx in range(len(thetas)):\n",
    "        thetas = tweak_theta_at_idx(examples, training_y, thetas, bias, idx, step_size)\n",
    "    bias = tweak_bias(examples, training_y, thetas, bias, step_size)\n",
    "    return (thetas, bias)\n",
    "\n",
    "# Now apply it to your ZERO_THETAS with a step size of 0.10. Print out what you get.\n",
    "new_thetas, new_bias = tweak_all_thetas(training_x, training_y, zero_thetas(), ZERO_BIAS, 0.10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.1, -0.1, 0.1, -0.1, 0.1, -0.1, 0.1, -0.1, -0.1, -0.1, 0.1, -0.1, -0.1, -0.1, 0.1, 0.1, 0.1, 0.1, -0.1]\n",
      "0.469144257827\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(new_thetas)\n",
    "print(\n",
    "    mean_absolute_error(\n",
    "        [logistic(z_score(x, new_thetas, new_bias)) for x in training_x],\n",
    "        training_y\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    mean_absolute_error(\n",
    "        [logistic(z_score(x, zero_thetas(), ZERO_BIAS)) for x in training_x],\n",
    "        training_y\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.469144257827\n",
      "0.439799326946\n",
      "0.412700986874\n",
      "0.389063143023\n",
      "0.368264503576\n",
      "0.350378609247\n",
      "0.334182431195\n",
      "0.320983105611\n",
      "0.309076373717\n",
      "0.298400956877\n"
     ]
    }
   ],
   "source": [
    "# Write a loop to tweak the thetas 10 times. Print out the MAE each time.\n",
    "\n",
    "thetas = zero_thetas()\n",
    "bias = ZERO_BIAS\n",
    "for _ in range(10):\n",
    "    thetas, bias = tweak_all_thetas(training_x, training_y, thetas, bias, 0.10)\n",
    "\n",
    "    print(\n",
    "        mean_absolute_error(\n",
    "            [logistic(z_score(x, thetas, bias)) for x in training_x],\n",
    "            training_y\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.798"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out how well your thetas do for classification.\n",
    "accuracy(\n",
    "    classify(\n",
    "        training_x,\n",
    "        thetas,\n",
    "        bias\n",
    "    ),\n",
    "    training_y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([-1, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1], -1)\n",
      "([0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1], -1)\n",
      "([0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1], -1)\n",
      "([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1], 1)\n",
      "([0, -1, -1, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1], -1)\n",
      "([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1], 1)\n",
      "([-1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1], -1)\n",
      "([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1], 1)\n",
      "([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0], 1)\n",
      "([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1], 1)\n"
     ]
    }
   ],
   "source": [
    "# Currently in order to tweak a theta, we much go through the\n",
    "# entire dataset to evaluate whether the change is good.\n",
    "# If we have twenty parameters, we must do this twenty times\n",
    "# before we have updated all the parameters.\n",
    "# In fact, we must do this *more* than once, in the case that\n",
    "# an increase is bad, we need to check about a decrease.\n",
    "# This problem will be worse with more parameters.\n",
    "#\n",
    "# Let's improve things by looking at a *single* example and\n",
    "# asking it to \"vote\" whether it is better to increase a\n",
    "# parameter or decrease it.\n",
    "#\n",
    "# If the example is a positive example, it wants to increase\n",
    "# the z value. If it is a negative example, it wants to\n",
    "# decrease the z value.\n",
    "#\n",
    "# If a feature is present, it will want to decrease or increase\n",
    "# the corresponding parameter. But if the feature is absent, it\n",
    "# has no vote for this parameter.\n",
    "#\n",
    "# Votes should be -1, 0, or +1.\n",
    "\n",
    "def calculate_example_votes(example, label):\n",
    "    vote_direction = (2 * label) - 1\n",
    "    votes = [\n",
    "        vote_direction * x\n",
    "        for x in example\n",
    "    ]\n",
    "    bias_vote = vote_direction\n",
    "    \n",
    "    return votes, bias_vote\n",
    "\n",
    "# Calculate and print the first ten votes:\n",
    "for idx in range(10):\n",
    "    print(\n",
    "        calculate_example_votes(training_x[idx], training_y[idx])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.126,  0.085, -0.048,  0.126, -0.14 ,  0.032, -0.136,  0.032,\n",
       "        -0.017, -0.051, -0.137,  0.107, -0.03 , -0.026, -0.03 ,  0.005,\n",
       "         0.007,  0.023,  0.062,  0.003]), -0.028)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a function that calculates the \"average\" vote.\n",
    "#\n",
    "# To keep track of sums of arrays, it is nice to use\n",
    "# numpy, which can vectorize additions.\n",
    "def calculate_all_votes(training_x, training_y):\n",
    "    total_votes = np.zeros(NUM_FEATURES)\n",
    "    total_bias_votes = 0.0\n",
    "    for (x, y) in zip(training_x, training_y):\n",
    "        votes, bias_vote = calculate_example_votes(x, y)\n",
    "        total_votes += np.array(votes)\n",
    "        total_bias_votes += bias_vote\n",
    "    return (\n",
    "        total_votes / len(training_x),\n",
    "        total_bias_votes / len(training_x)\n",
    "    )\n",
    "\n",
    "calculate_all_votes(training_x, training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46974722225\n",
      "0.709\n",
      "0.441238416745\n",
      "0.709\n",
      "0.415735849892\n",
      "0.709\n",
      "0.393827498095\n",
      "0.709\n",
      "0.375532830366\n",
      "0.708\n",
      "0.360528449487\n",
      "0.709\n",
      "0.34834697224\n",
      "0.709\n",
      "0.338502198417\n",
      "0.709\n",
      "0.330551137579\n",
      "0.709\n",
      "0.324116646217\n",
      "0.709\n",
      "[1.2599999999999998, 0.84999999999999987, -0.47999999999999993, 1.2599999999999998, -1.4000000000000004, 0.32000000000000006, -1.3600000000000003, 0.32000000000000006, -0.17000000000000004, -0.51000000000000001, -1.3700000000000001, 1.0700000000000001, -0.30000000000000004, -0.26000000000000001, -0.30000000000000004, 0.049999999999999996, 0.070000000000000007, 0.22999999999999995, 0.62000000000000011, 0.029999999999999995]\n"
     ]
    }
   ],
   "source": [
    "# Write a loop to ten times make the average voted update, printing the MAE each time.\n",
    "\n",
    "thetas = zero_thetas()\n",
    "bias = ZERO_BIAS\n",
    "for _ in range(10):\n",
    "    votes, bias_vote = calculate_all_votes(training_x, training_y)\n",
    "    thetas = [\n",
    "        theta + vote for (theta, vote) in zip(thetas, votes)\n",
    "    ]\n",
    "    bias += bias_vote\n",
    "\n",
    "    print(\n",
    "        mean_absolute_error(\n",
    "            [logistic(z_score(x, thetas, bias)) for x in training_x],\n",
    "            training_y\n",
    "        )\n",
    "    )\n",
    "    # Print the classification accuracy. This should be significantly lower than before.\n",
    "    print(\n",
    "        accuracy(\n",
    "            classify(\n",
    "                training_x,\n",
    "                thetas,\n",
    "                bias\n",
    "            ),\n",
    "            training_y\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Print the thetas. How are they related to the original thetas?\n",
    "print(thetas)\n",
    "\n",
    "# Try to run 100 iterations to see if your accuracy can improve the accuracy...\n",
    "# (You probably cannot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.126,  0.085, -0.048,  0.126, -0.14 ,  0.032, -0.136,  0.032,\n",
       "        -0.017, -0.051, -0.137,  0.107, -0.03 , -0.026, -0.03 ,  0.005,\n",
       "         0.007,  0.023,  0.062,  0.003]), -0.028000000000000001)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The problem is that we add the same vote all the time. Each example should vote\n",
    "# based on \"need.\" Examples that are well classified should vote less.\n",
    "# Or the flip side is that examples that can improve a lot from a change should vote more.\n",
    "\n",
    "def calculate_vote_weight(example, label, thetas, bias):\n",
    "    p = logistic(z_score(example, thetas, bias))\n",
    "    vote_weight = 2 * np.abs(label - p)\n",
    "    return vote_weight\n",
    "\n",
    "def calculate_all_votes2(training_x, training_y, thetas, bias):\n",
    "    total_votes = np.zeros(NUM_FEATURES)\n",
    "    total_bias_votes = 0.0\n",
    "    for (x, y) in zip(training_x, training_y):\n",
    "        votes, bias_vote = calculate_example_votes(x, y)\n",
    "        vote_weight = calculate_vote_weight(x, y, thetas, bias)\n",
    "        total_votes += np.array(votes) * vote_weight\n",
    "        total_bias_votes += bias_vote * vote_weight\n",
    "\n",
    "    return (\n",
    "        total_votes / len(training_x),\n",
    "        total_bias_votes / len(training_x)\n",
    "    )\n",
    "\n",
    "calculate_all_votes2(training_x, training_y, zero_thetas(), ZERO_BIAS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE\n",
      "0.493890064661\n",
      "ACCURACY\n",
      "0.708\n",
      "MAE\n",
      "0.488072699406\n",
      "ACCURACY\n",
      "0.759\n",
      "MAE\n",
      "0.482448056314\n",
      "ACCURACY\n",
      "0.776\n",
      "MAE\n",
      "0.477015678041\n",
      "ACCURACY\n",
      "0.778\n",
      "MAE\n",
      "0.471773796379\n",
      "ACCURACY\n",
      "0.783\n",
      "MAE\n",
      "0.466719529663\n",
      "ACCURACY\n",
      "0.788\n",
      "MAE\n",
      "0.461849089341\n",
      "ACCURACY\n",
      "0.79\n",
      "MAE\n",
      "0.457157967869\n",
      "ACCURACY\n",
      "0.788\n",
      "MAE\n",
      "0.45264110609\n",
      "ACCURACY\n",
      "0.789\n",
      "MAE\n",
      "0.448293039603\n",
      "ACCURACY\n",
      "0.789\n",
      "MAE\n",
      "0.444108024597\n",
      "ACCURACY\n",
      "0.786\n",
      "MAE\n",
      "0.44008014436\n",
      "ACCURACY\n",
      "0.788\n",
      "MAE\n",
      "0.436203398028\n",
      "ACCURACY\n",
      "0.788\n",
      "MAE\n",
      "0.432471773414\n",
      "ACCURACY\n",
      "0.791\n",
      "MAE\n",
      "0.428879305799\n",
      "ACCURACY\n",
      "0.791\n",
      "MAE\n",
      "0.425420124539\n",
      "ACCURACY\n",
      "0.79\n",
      "MAE\n",
      "0.422088489253\n",
      "ACCURACY\n",
      "0.79\n",
      "MAE\n",
      "0.418878817184\n",
      "ACCURACY\n",
      "0.791\n",
      "MAE\n",
      "0.41578570319\n",
      "ACCURACY\n",
      "0.791\n",
      "MAE\n",
      "0.412803933635\n",
      "ACCURACY\n",
      "0.794\n",
      "MAE\n",
      "0.409928495283\n",
      "ACCURACY\n",
      "0.794\n",
      "MAE\n",
      "0.407154580153\n",
      "ACCURACY\n",
      "0.794\n",
      "MAE\n",
      "0.404477587137\n",
      "ACCURACY\n",
      "0.795\n",
      "MAE\n",
      "0.401893121061\n",
      "ACCURACY\n",
      "0.795\n",
      "MAE\n",
      "0.399396989759\n",
      "ACCURACY\n",
      "0.795\n",
      "MAE\n",
      "0.396985199632\n",
      "ACCURACY\n",
      "0.795\n",
      "MAE\n",
      "0.394653950072\n",
      "ACCURACY\n",
      "0.795\n",
      "MAE\n",
      "0.392399627072\n",
      "ACCURACY\n",
      "0.795\n",
      "MAE\n",
      "0.390218796278\n",
      "ACCURACY\n",
      "0.797\n",
      "MAE\n",
      "0.388108195686\n",
      "ACCURACY\n",
      "0.797\n",
      "MAE\n",
      "0.386064728145\n",
      "ACCURACY\n",
      "0.796\n",
      "MAE\n",
      "0.384085453803\n",
      "ACCURACY\n",
      "0.796\n",
      "MAE\n",
      "0.382167582593\n",
      "ACCURACY\n",
      "0.796\n",
      "MAE\n",
      "0.380308466824\n",
      "ACCURACY\n",
      "0.796\n",
      "MAE\n",
      "0.378505593962\n",
      "ACCURACY\n",
      "0.796\n",
      "MAE\n",
      "0.376756579617\n",
      "ACCURACY\n",
      "0.796\n",
      "MAE\n",
      "0.37505916078\n",
      "ACCURACY\n",
      "0.796\n",
      "MAE\n",
      "0.373411189326\n",
      "ACCURACY\n",
      "0.796\n",
      "MAE\n",
      "0.371810625802\n",
      "ACCURACY\n",
      "0.797\n",
      "MAE\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-1f522cfa941a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     print(\n\u001b[1;32m     17\u001b[0m         mean_absolute_error(\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mtraining_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         )\n",
      "\u001b[0;32m<ipython-input-33-1f522cfa941a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m     print(\n\u001b[1;32m     17\u001b[0m         mean_absolute_error(\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mtraining_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         )\n",
      "\u001b[0;32m<ipython-input-4-999c28a81a28>\u001b[0m in \u001b[0;36mz_score\u001b[0;34m(example, thetas, bias)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_FEATURES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mthetas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Write a loop to ten times make the average voted update, printing the MAE each time.\n",
    "\n",
    "thetas = zero_thetas()\n",
    "bias = ZERO_BIAS\n",
    "\n",
    "LEARNING_RATE = 0.2\n",
    "\n",
    "for _ in range(100):\n",
    "    votes, bias_vote = calculate_all_votes2(training_x, training_y, thetas, bias)\n",
    "    thetas = [\n",
    "        theta + vote * LEARNING_RATE for (theta, vote) in zip(thetas, votes)\n",
    "    ]\n",
    "    bias += bias_vote * LEARNING_RATE\n",
    "\n",
    "    print(\"MAE\")\n",
    "    print(\n",
    "        mean_absolute_error(\n",
    "            [logistic(z_score(x, thetas, bias)) for x in training_x],\n",
    "            training_y\n",
    "        )\n",
    "    )\n",
    "    # Print the classification accuracy. This should be significantly lower than before.\n",
    "    print(\"ACCURACY\")\n",
    "    print(\n",
    "        accuracy(\n",
    "            classify(\n",
    "                training_x,\n",
    "                thetas,\n",
    "                bias\n",
    "            ),\n",
    "            training_y\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Print the thetas. How are they related to the original thetas?\n",
    "print(thetas)\n",
    "\n",
    "# Try to run 100 iterations to see if your accuracy can improve the accuracy...\n",
    "# (You probably cannot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
